AI HR Policy Assistant

Hybrid RAG System with FAISS + BM25 + LoRA

An internal AI assistant that answers company HR and policy questions with citations, built using a production-grade Retrieval-Augmented Generation (RAG) architecture.

This system prioritizes correctness, transparency, and controllability over raw model intelligence.

Key Capabilities

ğŸ“„ PDF ingestion & semantic chunking

ğŸ” Hybrid retrieval

FAISS (semantic similarity, cosine search)

BM25 (lexical keyword matching)

âš–ï¸ Score fusion: final = Î±Â·semantic + (1âˆ’Î±)Â·lexical

ğŸ“ Citation-based answers with page navigation

ğŸ§  LoRA fine-tuned generation (style & tone only â€” not facts)

ğŸ“Š Admin dashboard (stats, logs, index management)

ğŸ§ª Evaluation pipeline (precision, recall, MRR)

ğŸ” JWT-protected admin APIs

âš™ï¸ Async background indexing

ğŸ—ï¸ System Architecture (High Level)
User Question
     â†“
Hybrid Retrieval
  â”œâ”€ FAISS (semantic vectors)
  â””â”€ BM25 (lexical tokens)
     â†“
Score Fusion (Î± = 0.1)
     â†“
Top-K Chunks
     â†“
Context Builder
     â†“
LLM + LoRA (style only)
     â†“
Answer + Citations

ğŸ§  Why Hybrid Retrieval?

HR and policy documents are lexical by nature â€” exact wording matters.

Pure semantic search often retrieves related but incorrect clauses.

This system combines:

Semantic similarity â†’ understands natural language

Lexical precision â†’ respects exact policy language

After evaluation, Î± = 0.1 delivered the best precision@1 and MRR, confirming that lexical signals should dominate in this domain.

ğŸ§© FAISS Position â†” Chunk Mapping (Important)

FAISS stores only vectors, not metadata.

To reliably map search results back to documents:

Each FAISS index has a corresponding registry entry

The registry stores an ordered mapping:

faiss_position â†’ chunk_id


This guarantees deterministic retrieval even after:

restarts

index merges

background re-indexing

ğŸ§  LoRA Fine-Tuning Philosophy

LoRA is used only to control response style, not to inject knowledge.

âŒ No factual learning

âŒ No hallucination tolerance

âœ… Professional HR tone

âœ… Structured, concise answers

If the LoRA adapter fails to load, the backend fails fast instead of silently degrading.

ğŸ—‚ï¸ Repository Structure (Simplified)
backend/
 â”œâ”€â”€ app/
 â”‚   â”œâ”€â”€ api/              # FastAPI routes
 â”‚   â”œâ”€â”€ core/             # RAG + retrieval logic
 â”‚   â”œâ”€â”€ services/         # Indexer, evaluator, LoRA loader
 â”‚   â”œâ”€â”€ models/           # SQLModel DB schemas
 â”‚   â”œâ”€â”€ repos/            # DB access layer
 â”‚   â””â”€â”€ utils/            # FAISS utilities
 â”‚
 â”œâ”€â”€ lora_models/          # LoRA adapters
 â””â”€â”€ data/                 # PDFs, FAISS indexes, eval sets

frontend/
 â”œâ”€â”€ app/                  # Next.js pages
 â”œâ”€â”€ components/           # UI components
 â””â”€â”€ lib/                  # API helpers

ğŸ§ª Evaluation

The system includes a simple but effective evaluation pipeline:

Upload labeled Q&A datasets (JSON)

Run evaluation asynchronously

Metrics stored in DB:

precision-like overlap scoring

per-question retrieval inspection

overall score & runtime

Evaluation is treated as a first-class citizen, not an afterthought.

ğŸ› ï¸ Tech Stack

Backend

FastAPI

SQLModel + SQLite

FAISS

Sentence Transformers

Transformers + PEFT (LoRA)

PyMuPDF

Frontend

Next.js (App Router)

Tailwind CSS

PDF.js

TypeAnimation

ğŸ¯ Design Principles

Retrieval > Generation

Transparency over magic

Evaluation before optimization

Models are unreliable â€” systems must compensate

Fail fast instead of failing silently

ğŸ“Œ Status

âœ”ï¸ End-to-end functional

âœ”ï¸ Retrieval evaluated

âœ”ï¸ Admin observability implemented

ğŸš« Deployment details intentionally omitted

ğŸ“ Notes

This project was built to understand real-world RAG system design, not to showcase prompt tricks.

If youâ€™re interested in production AI engineering, hybrid retrieval, or safe LLM systems â€” this codebase is meant to be read, not just run.
